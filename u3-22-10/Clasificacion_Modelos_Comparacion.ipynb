{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c5f68203",
      "metadata": {
        "id": "c5f68203"
      },
      "source": [
        "\n",
        "# Clasificaci√≥n Supervisada ‚Äî Comparaci√≥n de Modelos\n",
        "\n",
        "**Objetivos**\n",
        "- Comprender los fundamentos de la **clasificaci√≥n supervisada**.\n",
        "- Conocer y aplicar diferentes modelos de clasificaci√≥n:\n",
        "  - Naive Bayes  \n",
        "  - K-Vecinos m√°s Cercanos (KNN)  \n",
        "  - √Årbol de Decisi√≥n  \n",
        "  - Bosque Aleatorio  \n",
        "  - Red Neuronal (MLP)\n",
        "- Comparar el rendimiento de los modelos y reflexionar sobre sus ventajas y limitaciones.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1dea37c6",
      "metadata": {
        "id": "1dea37c6"
      },
      "source": [
        "\n",
        "## 1Ô∏è‚É£ Conceptos clave\n",
        "\n",
        "### Clasificaci√≥n supervisada\n",
        "El modelo aprende a **asignar una clase (C)** a un conjunto de caracter√≠sticas (**X**) a partir de ejemplos conocidos.\n",
        "\n",
        "Flujo general:\n",
        "1. Entrenar con datos conocidos ‚Üí `fit(X_train, y_train)`\n",
        "2. Predecir con datos nuevos ‚Üí `predict(X_test)`\n",
        "3. Evaluar rendimiento ‚Üí `score()` o `accuracy_score()`\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ Naive Bayes\n",
        "Se basa en el **teorema de Bayes** y supone independencia entre las variables:\n",
        "\\[\n",
        "P(C|X) = \\frac{P(C)P(X|C)}{P(X)}\n",
        "\\]\n",
        "Ventajas: r√°pido, simple y eficiente en datos de texto o con muchas variables.\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ √Årbol de Decisi√≥n\n",
        "Crea reglas del tipo **\"si-entonces\"** para clasificar los datos.  \n",
        "Ventaja: interpretabilidad.  \n",
        "Inconveniente: puede sobreajustarse.\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ Bosque Aleatorio\n",
        "Conjunto de muchos √°rboles de decisi√≥n.  \n",
        "Reduce el **sobreajuste** y mejora la estabilidad.\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ K-Vecinos m√°s Cercanos (KNN)\n",
        "Clasifica observando las **k muestras m√°s cercanas**.  \n",
        "No requiere entrenamiento, pero puede ser lento con grandes vol√∫menes de datos.\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ Redes Neuronales (MLP)\n",
        "Modelo con **capas conectadas** (entrada, ocultas, salida).  \n",
        "Capaz de modelar relaciones **no lineales**.  \n",
        "Se introduce aqu√≠ solo para comparar resultados.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9beda2d2",
      "metadata": {
        "id": "9beda2d2"
      },
      "source": [
        "\n",
        "## 2Ô∏è‚É£ Pr√°ctica guiada: Comparaci√≥n de clasificadores en el dataset Iris\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "71d9d7d0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71d9d7d0",
        "outputId": "4f06b39b-9284-4015-da83-7cb946ef15bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultados de precisi√≥n en el conjunto de test:\n",
            "SVM: 0.9467\n",
            "Naive Bayes: 0.9467\n",
            "LDA: 0.9600\n",
            "QDA: 0.9600\n",
            "Decision Tree: 0.9600\n",
            "Random Forest: 0.9467\n",
            "K-Nearest: 0.9600\n",
            "Neural Network: 0.9867\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import svm\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# 1Ô∏è‚É£ Cargar datos\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)\n",
        "\n",
        "# 2Ô∏è‚É£ Definir modelos\n",
        "models = {\n",
        "    \"SVM\": svm.SVC(),\n",
        "    \"Naive Bayes\": GaussianNB(),\n",
        "    \"LDA\": LinearDiscriminantAnalysis(),\n",
        "    \"QDA\": QuadraticDiscriminantAnalysis(),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(),\n",
        "    \"Random Forest\": RandomForestClassifier(),\n",
        "    \"K-Nearest\": KNeighborsClassifier(),\n",
        "    \"Neural Network\": MLPClassifier(alpha=1, max_iter=1000)\n",
        "}\n",
        "\n",
        "# 3Ô∏è‚É£ Entrenar y evaluar\n",
        "print(\"Resultados de precisi√≥n en el conjunto de test:\")\n",
        "scores = {}\n",
        "for name, clf in models.items():\n",
        "    clf.fit(X_train, y_train)\n",
        "    score = clf.score(X_test, y_test)\n",
        "    scores[name] = score\n",
        "    print(f\"{name}: {score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15df89b2",
      "metadata": {
        "id": "15df89b2"
      },
      "source": [
        "\n",
        "### üîç Visualizaci√≥n comparativa de resultados\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0c5061b",
      "metadata": {
        "id": "a0c5061b"
      },
      "outputs": [],
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.barh(list(scores.keys()), list(scores.values()), color='teal')\n",
        "plt.xlabel(\"Precisi√≥n (accuracy)\")\n",
        "plt.title(\"Comparaci√≥n de clasificadores - Dataset Iris\")\n",
        "plt.xlim(0.8, 1.0)\n",
        "plt.grid(axis='x', linestyle='--', alpha=0.5)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ad2fa20",
      "metadata": {
        "id": "6ad2fa20"
      },
      "source": [
        "\n",
        "## 3Ô∏è‚É£ Actividades propuestas\n",
        "\n",
        "1. Cambia el tama√±o del conjunto de test (`test_size`) a 0.2 y 0.4. ¬øVar√≠a la precisi√≥n?\n",
        "2. Prueba diferentes par√°metros:\n",
        "   - `n_neighbors` en KNN (3, 5, 10)\n",
        "   - `max_depth` en √Årbol de Decisi√≥n (2, 4, None)\n",
        "   - `n_estimators` en Random Forest (50, 100, 200)\n",
        "3. Ejecuta el mismo c√≥digo con otro dataset, como `load_breast_cancer()`.\n",
        "4. Completa esta tabla en tu cuaderno:\n",
        "\n",
        "| Modelo | Idea b√°sica | Par√°metro clave | Ventaja | Limitaci√≥n |\n",
        "|---------|--------------|----------------|----------|-------------|\n",
        "| Naive Bayes |  |  |  |  |\n",
        "| √Årbol |  |  |  |  |\n",
        "| Bosque Aleatorio |  |  |  |  |\n",
        "| KNN |  |  |  | |\n",
        "| Red Neuronal | |  |  | |\n",
        "\n",
        "5. Reflexiona:\n",
        "   - ¬øPor qu√© algunos modelos funcionan mejor que otros?\n",
        "   - ¬øCu√°l ser√≠a m√°s adecuado si necesitas rapidez?\n",
        "   - ¬øY si necesitas explicabilidad?\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}